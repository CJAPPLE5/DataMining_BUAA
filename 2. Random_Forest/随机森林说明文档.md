# 随机森林python实现

赵梁煊 17373157 170617

## 一、如何运行

在main中，设置了train变量来指明当前是否是训练。若是，则进入训练程序；否则，进入预测程序。

训练程序中，先读入x, y，然后划分训练集和验证集。然后设置模型参数并训练，最后计算在训练集、验证集上的准确度和f1得分，并保存模型。训练程序也实现了参数调优的功能。

测试程序中，先加载模型，然后进行预测，最终将结果输出。

模型中的random_state参数可以保证，每次运行时每棵树的样本集不会改变，保证了实验的可重复性。

```python
if __name__ == '__main__':
    train = True
    if train:
        x = pd.read_csv("x_train.csv").drop('index', axis=1).reset_index(drop=True)
        y = pd.read_csv("y_train.csv").drop('index', axis=1).reset_index(drop=True)
        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=1)
        for i in [x_train, x_val, y_train, y_val]:
            i = i.reset_index(drop=True)
        parameter = []
        n_estimators_all = [i for i in range(5, 50, 2)]
        max_depth_all = [i for i in range(5, 30, 2)] + [-1]
        min_samples_split_all = [i for i in range(2, 20, 2)]
        min_samples_leaf_all = [i for i in range(1, 9, 2)]
        min_split_entropy_all = [0.0, 0.05, 0.1, 0.15, 0.2]
        colsample_all = [0.2, 0.35, 0.5, 0.65, 0.8]
        rowsample_all = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]
        for (n_estimators, max_depth, min_samples_split, min_samples_leaf, min_split_entropy, colsample, rowsample) in itertools.product(n_estimators_all, max_depth_all, min_samples_split_all, min_samples_leaf_all, min_split_entropy_all, colsample_all, rowsample_all):
            print('------parameters: ', n_estimators, max_depth, min_samples_split, min_samples_leaf, min_split_entropy, colsample, rowsample)
            clf = RandomForestClassifier(n_estimators=n_estimators,
                                         max_depth=max_depth,
                                         min_samples_split=min_samples_split,
                                         min_samples_leaf=min_samples_leaf,
                                         min_split_entropy=min_split_entropy,
                                         colsample=colsample,
                                         rowsample=rowsample,
                                         random_state=413)
            clf.fit(x_train, y_train)
            save_model(clf, 'MyRfModel.txt')
            train_acc = metrics.accuracy_score(y_train, clf.predict(x_train))
            val_acc = metrics.accuracy_score(y_val, clf.predict(x_val))
            train_f1 = metrics.f1_score(y_train, clf.predict(x_train))
            val_f1 = metrics.f1_score(y_val, clf.predict(x_val))
            print('train acc: ', train_acc)
            print('val acc: ', val_acc)
            print('train f1: ', train_f1)
            print('val f1: ', val_f1)
            if val_acc > 0.97 and val_f1 > 0.97:
                parameter.append([n_estimators, max_depth, min_samples_split, min_samples_leaf, min_split_entropy, colsample, rowsample, train_acc, val_acc, train_f1, val_f1])
        pd.DataFrame(np.array(parameter), columns=['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_split_entropy', 'colsample_bytree', 'rowsample', 'train_acc', 'val_acc', 'train_f1', 'val_f1']).to_csv('parameter.csv')
    else:
        clf = load_model('MyRfModel.txt')
        x = pd.read_csv("x_train.csv").drop('index', axis=1).reset_index(drop=True)
        y = pd.read_csv("y_train.csv").drop('index', axis=1).reset_index(drop=True)
        array = clf.predict(x)
        print('test acc: ', metrics.accuracy_score(y, array))
        print('test f1: ', metrics.f1_score(y, array))
        df = pd.DataFrame(array, columns=['label'])
        df.to_csv('prediction.csv', index_label=None, index=False)

```

## 二、流程介绍

### 1. 决策树与随机森林的定义

#### (1) 决策树结点类

决策树是随机森林的基本单位，许多独立的决策树一起构成了随机森林。该类定义了决策树的结点：若当前结点是叶子结点，那么分类特征、特征分点位置和左子树、右子树属性均为None，仅有一个值表示该叶子结点的类别（1或-1）；若当前结点不是叶子结点，则相反，按照分类特征和特征的分点位置，可以将当前结点中的样本分成两棵子树，但该结点就没有类别了。

由于我们决策树的结构是这样定义的，所以在预测和打印的时候都是从根节点开始递归遍历的。

```python
class TreeNode(object):
    """定义决策树中的一个结点"""
    def __init__(self):
        self.split_feature = None # 若不为叶子结点，则代表分类特征；否则为None
        self.split_value = None   # 若不为叶子结点，则代表该分类特征的二分点位置
        self.left_tree = None     # 若不为叶子结点，则为左子树；否则为None
        self.right_tree = None    # 若不为叶子结点，则为右子树；否则为None
        self.leaf_label = None    # 若为叶节点，则代表该叶节点的分类；否则为None
    def calc_predict_value(self, features):
        """递归寻找样本所属的叶子节点（即预测分类）"""
        # 为叶子结点
        if self.leaf_label is not None:
            return self.leaf_label
        # 不为叶子结点，当前样本该特征取值小于二分点，去左子树寻找
        elif features[self.split_feature] <= self.split_value:
            return self.left_tree.calc_predict_value(features)
        # 不为叶子结点，当前样本该特征取值大于二分点，去右子树寻找
        else:
            return self.right_tree.calc_predict_value(features)
```

#### （2）随机森林类

随机森林分类器由许多个独立的决策树构成，其中n_estimators表示决策树的个数，random_state随机种子决定了样本划分的方法，是随机森林的参数；其余的参数均可以看作是决策树的参数，分别表示了树的深度、节点分裂所需的最小样本数量、叶子节点最少样本数量、分裂所需的最小熵、列采样比例、行采样比例。

最后还有两个成员变量trees和feature_importances_ ：trees是所有决策树的根节点，feature_importances_是所有特征的重要程度。

```python
class RandomForestClassifier(object):
    def __init__(self, n_estimators=10, max_depth=-1, min_samples_split=2, min_samples_leaf=1,
                 min_split_entropy=0.0, colsample=0.5, rowsample=0.8, random_state=None):
        """
        随机森林参数
        ----------
        n_estimators:      树数量
        max_depth:         树深度，-1表示不限制深度
        min_samples_split: 节点分裂所需的最小样本数量，小于该值节点终止分裂
        min_samples_leaf:  叶子节点最少样本数量，小于该值叶子被合并
        min_split_gain:    分裂所需的最小增益，小于该值节点终止分裂
        colsample:         列采样比例
        subsample:         行采样比例
        random_state:      随机种子，设置之后每次生成的n_estimators个样本集不会变，确保实验可重复
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth if max_depth != -1 else float('inf')
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_split_entropy = min_split_entropy
        self.colsample = colsample
        self.rowsample = rowsample
        self.random_state = random_state
        self.trees = dict()
        self.feature_importances_ = dict()
```

### 2. 模型训练

**fit()函数**是模型训练的入口。首先要设立随机种子，并根据输入计算行采样和列采样的具体数量。然后开始逐个构建决策树：选择抽样的行（样本）与列（特征），调用_fit()函数递归地构建决策树，并将根节点tree_root放入trees字典中。

```python
class RandomForestClassifier(object):
    def fit(self, features, label):
        """模型训练入口"""
        if self.random_state:
            random.seed(self.random_state)
        # 从range(self.n_estimators)这个list中返回self.n_estimators个元素组成的list
        random_state_stages = random.sample(range(self.n_estimators), self.n_estimators)
        # 按照行列采样比例，设置行列采样数量
        self.colsample = int(self.colsample * len(features.columns))
        self.rowsample = int(self.rowsample * len(features))
        for tree_num in range(self.n_estimators):
            print(("tree_num: " + str(tree_num+1)).center(40, '-'))
            # 随机选择行和列，每棵树均设置不同的seed保证行列选择都是不同的
            random.seed(random_state_stages[tree_num])
            random_row_indexes = random.sample(range(len(features)), int(self.rowsample))   # 行
            random_col_indexes = random.sample(features.columns.tolist(), self.colsample)   # 列
            features_random = features.loc[random_row_indexes, random_col_indexes].reset_index(drop=True)
            label_random = label.loc[random_row_indexes, :].reset_index(drop=True)
            tree_root = self._fit(features_random, label_random, depth=0)
            self.trees[tree_num] = tree_root
```

**_fit()函数**用于递归建立决策树。若当前结点下的样本不满足分裂的条件，或树的深度超过限制，则该结点作为叶子结点，计算叶子结点的标签。

否则认为当前结点可以分裂，**调用best_split()函数**寻找使得分裂后增益最大的分类特征、特征二分点、交叉熵，再根据找到的这些信息，将样本分到左右子树。若左子树或者右子树中有哪个不满足叶子结点样本要求的最小样本数量，则也不分裂，将当前结点当做叶子结点；否则就可以分裂成左右两个子树，再递归调用_fit()来处理两个子树了。

```python
class RandomForestClassifier(object):
    def _fit(self, features, label, depth):
        """递归建立决策树"""
        tree_node = TreeNode()
        # 如果当前节点下的样本不需要分类（类别全都一样/样本小于分裂要求的最小样本数量），
        # 或者树的深度超过最大深度，则选取出现次数最多的类别。终止分裂
        if (len(label['label'].unique()) <= 1 or len(label) <= self.min_samples_split) \
                or (depth >= self.max_depth):
            tree_node.leaf_label = self.calc_leaf_value(label['label'])
            return tree_node
        # 当前结点需要分裂
        left_features, right_features, left_label, right_label, best_split_feature, best_split_value, best_split_entropy = self.best_split(features, label)
        # 如果父节点分裂后，左叶子节点/右叶子节点样本小于设置的叶子节点最小样本数量，则该父节点终止分裂
        if len(left_features) <= self.min_samples_leaf or \
                len(right_features) <= self.min_samples_leaf or \
                best_split_entropy <= self.min_split_entropy:
            tree_node.leaf_label = self.calc_leaf_value(label['label'])
            return tree_node
        # 否则可以分裂
        else:
            # 如果分裂的时候用到该特征，则该特征的importance加1；并为属于非叶子结点的属性赋值
            self.feature_importances_[best_split_feature] = self.feature_importances_.get(best_split_feature, 0) + 1
            tree_node.split_feature = best_split_feature
            tree_node.split_value = best_split_value
            tree_node.left_tree = self._fit(left_features, left_label, depth + 1)
            tree_node.right_tree = self._fit(right_features, right_label, depth + 1)
            return tree_node
```

**best_split()函数**用于寻找最好的数据集划分方式，找到最优分裂特征、分裂点、分裂交叉熵。遍历所有的特征，构建每个特征可能的分位点列表，再遍历分位点列表的每一个点，若这样分更优，则更新分裂方式。然后，根据最优的分裂特征和分裂点划分样本为左右子树。其中，计算分裂交叉熵**用到了calc_gini()函数**，计算单棵子树的gini后加权求和作为分裂后该子树的熵。

```python
class RandomForestClassifier(object):
    def best_split(self, features, label):
        """寻找最好的样本划分方式，即最优的分裂特征、分裂点、分裂交叉熵"""
        """并根据该方式其对样本进行划分"""
        best_split_feature = None
        best_split_value = None
        best_split_entropy = 1
        for feature in features.columns:
            # 训练集每个维度的特征个数都不多，可以直接选取它们作为分位点
            split_values_list = sorted(features[feature].unique().tolist())
            # 对可能的分裂阈值求分裂增益，选取增益最大的阈值
            for split_value in split_values_list:
                left_label = label[features[feature] <= split_value]
                right_label = label[features[feature] > split_value]
                left_ratio = 1.0 * len(left_label) / len(label)
                right_ratio = 1.0 * len(right_label) / len(label)
                split_entropy = self.calc_gini(left_label['label'], left_ratio) + \
                                self.calc_gini(right_label['label'], right_ratio)
                if split_entropy < best_split_entropy:
                    best_split_feature = feature
                    best_split_value = split_value
                    best_split_entropy = split_entropy
        # 划分样本
        left_features = features[features[best_split_feature] <= best_split_value]
        left_label = label[features[best_split_feature] <= best_split_value]
        right_features = features[features[best_split_feature] > best_split_value]
        right_label = label[features[best_split_feature] > best_split_value]
        return left_features, right_features, left_label, right_label, \
               best_split_feature, best_split_value, best_split_entropy
```

**calc_gini()函数**是采用基尼指数来近似估计交叉熵的。计算子树的gini，然后再乘以子树占父结点样本数的比例。

```python
class RandomForestClassifier(object):
    def calc_gini(targets, ratio):
        """采用基尼指数作为交叉熵的近似估计，计算每棵子树的基尼指数"""
        gini = 1
        label_counts = collections.Counter(targets)
        for key in label_counts:
            prob = label_counts[key] * 1.0 / len(targets)
            gini -= prob ** 2
        return gini * ratio
```

### 3. 模型预测

**predict()函数**用来预测输入样本的标签。每棵树独立预测，投票决定随机森林的预测结果。其中每棵树的预测都条用了**TreeNode类中的calc_predict_value()函数**从根结点开始递归查询。

```python
class RandomForestClassifier(object): 
    def predict(self, features):
        """输入样本，预测所属类别"""
        pred_result = []
        for _, row in features.iterrows():
            trees_pred = []
            # 每棵树独立预测，投票决定预测的标签
            for _, tree in self.trees.items():
                trees_pred.append(tree.calc_predict_value(row))
            pred_label_counts = collections.Counter(trees_pred)
            most_key = 1
            most_count = 0
            for key in pred_label_counts.keys():
                if pred_label_counts[key] > most_count:
                    most_key = key
                    most_count = pred_label_counts[key]
            pred_result.append(most_key)
        return np.array(pred_result)
class TreeNode(object):
    def calc_predict_value(self, features):
        """递归寻找样本所属的叶子节点（即预测分类）"""
        # 为叶子结点
        if self.leaf_label is not None:
            return self.leaf_label
        # 不为叶子结点，当前样本该特征取值小于二分点，去左子树寻找
        elif features[self.split_feature] <= self.split_value:
            return self.left_tree.calc_predict_value(features)
        # 不为叶子结点，当前样本该特征取值大于二分点，去右子树寻找
        else:
            return self.right_tree.calc_predict_value(features)
```

### 4. 模型导出与导入

利用**pickle的dump()和load()**来导出、导入模型（RandomForestClassifier类的实例clf）。

```python
def save_model(RfClf, filename):
    file = open(filename, 'wb')
    pickle.dump(RfClf, file)
    file.close()
def load_model(filename):
    file = open(filename, 'rb')
    return pickle.load(file)
```

